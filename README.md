

<div align="center">


<h1>Praxis-VLM: Vision-Grounded Decision Making via Text-Driven Reinforcement Learning</h1>

<div>
    <a target='_blank'>Zhe Hu<sup>1</sup></a>&emsp;
    <a target='_blank'>Jing Li<sup>1</sup></a>&emsp;
    <a target='_blank'>Zhongzhu Pu<sup>2,3</sup></a>&emsp;
    <a target='_blank'>Hou Pong Chan<sup>4</sup></a>&emsp;
    <a target='_blank'>Yu Yin<sup>5</sup></a>
</div>

<div>
    <sup>1</sup>The Hong Kong Polytechnic University, <sup>2</sup>Tsinghua University, <sup>3</sup>InspireOmni AI&emsp; 
</div>
<sup>4</sup>Alibaba Group, <sup>5</sup>Case Western Reserve University
<div>
</div>

[[Arxiv]](https://arxiv.org/pdf/2503.16965v2)
[[ðŸ¤— HuggingFace]](https://huggingface.co/collections/zhehuderek/praxis-vlm-67f5d8b3e077bdde7ec24baa)

---

</div>


#### ðŸŒŸ This Repo contains code and data for Praxis-VLM, which leverages textual GRPO training for vision-grounded decison making.

## Overview
We introduce Praxis-VLM, a reasoning VLM for vision-grounded decision-making. Praxis-VLM employs the GRPO algorithm on textual scenarios to instill robust reasoning capabilities. These reasoning skills, acquired purely from text, successfully transfer to multimodal inference with visual inputs, significantly reducing reliance on scarce paired image-text training data.

<div align='left'><img src="./assets/intro_figure.jpg"  alt="NAME" width="60%"/></div>


